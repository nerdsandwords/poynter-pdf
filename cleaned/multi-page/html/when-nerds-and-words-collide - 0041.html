<body>
 <p>
  federal taxpayers the cost of the S&amp;L bailout. To pull off the project, we needed data on the original book value of assets held at the time any given S&amp;L went into bankruptcy; the reassessed value placed on the assets upon acquisition by the RTC; a sale price for any assets sold; and the names of the purchasers. As a starting point, we set up a meeting to interview the RTC’s director about what data he had and how it was kept.
 </p>
 <p>
  At that initial meeting Bob and I were told that no database existed. Depending upon one’s point of view that was technically the truth. While the centralized database was under construction, there were four regional databases at the time of our interview, not one national database of assets. Following that frustrating meeting we found a source within the agency who brought us up to speed, and armed with the proper terminology we were able to construct what we thought was an ironclad Freedom of Information request.
 </p>
 <p>
  Nine months later we were informed that our request had been filled, and the next thing we knew, we were the proud owners of roughly 45,000 pieces of paper. The RTC had simply dumped the contents of all four regional computers, which satisfied the requirements of federal FOIA to provide us with “the record.”
 </p>
 <p>
  Rekeying that data into a usable database cost the
  <em>
   Los Angeles Times
  </em>
  roughly $20,000, and I doubt seriously whether any other news organization would have bothered. I doubt seriously whether the
  <em>
   Times
  </em>
  would do so today.
 </p>
 <p>
  Armed with the data, we began to examine the quality of information we had been given. It was readily apparent that roughly one-quarter of the information required for the analysis was missing - some properties lacked an original book value, while others lacked a reassessed value, while still others lacked a sale price. Some properties lacked all three values. Given that the outcome of the study might be embarrassing or even politically damaging to the RTC, we decided that at least half the missing data had to be retrieved before we could proceed. Over the next month, the paper spent roughly $
  10,000
  on property searches throughout the country. Once we knocked the missing data down to no more than
  10
  percent in any region, we proceeded with the analysis.
 </p>
 <h4 id="bookmark0">
  Realize that data require a closer look
 </h4>
 <p>
  During the final analysis of RTC records, which prompted a GAO investigation and ultimately led to a $40 billion reduction in one of the RTC’s funding requests, we discovered that the agency’s bookkeeping methods made it appear as though it were recouping substantially more of the bailout costs than was actually the case.
 </p>
 <p>
  To facilitate the sale of less desirable property, the agency had begun bundling one or more quality assets together with properties it could not give away under normal circumstances. While 10 properties might collectively sell for $500,000, nine might be nearly worthless. The RTC would then put the $500,000 sale price on each of the individual properties included in the bundle without notation, making it look to the casual observer as though the total purchase price of the bundle was $5 million. Once we caught on to the bookkeeping shuffle, the agency’s performance looked far less rosy.
 </p>
 <h4 id="bookmark1">
  Where possible, live with your data
 </h4>
 <p>
  This is a luxury that few journalists can afford, particularly those at small newspapers or local television stations where they are asked to become experts in a day and allowed to focus on any one subject for a few days at most. Having said that, there is no substitute for getting to know every quirk in the data you have opted to analyze.
 </p>
 <p>
  Upon deciding to leave the
  <em>
   Los Angeles Times
  </em>
  to form our own consulting company in January 1996, my partner and I made a conscious decision to abandon temporarily any data-based work that did not in some way touch upon our expertise in covering campaigns. Having steeped ourselves in federal contribution data for six years, we knew we were in a unique position to spot trends others might not see.
 </p>
 <p>
  The first such trend led our old employer to break a series of stories detailing the Democratic National Committee’s acceptance of foreign contributions and other questionable fundraising practices. It arose because we suddenly began seeing dozens of first-time soft money contributors and took the time to discover that at least one, Cheong Am America Inc., had no base of operations within the United States. While that first story was buried deep inside a Saturday edition, the series it spawned eventually won an IRE prize.
 </p>
 <h4 id="bookmark2">
  Develop an analysis plan, even if you change it
 </h4>
 <p>
  Every data analysis should begin with a story idea or hypothesis. Whenever possible, that hypothesis should grow from initial reporting. Only then should one look for data to sharpen the story’s focus - to prove or disprove the hypothesis in social science terms. Fishing expeditions are, in my experience, a complete waste of time.
 </p>
 <p>
  Do not assume you have been given the data you requested. While that sounds absurd, running a few simple checks on the data can save you the embarrassment of labeling your city the violent crime capital of the world when the database you are analyzing does not include Washington, D.C., and several other major metropolitan areas.
 </p>
 <p>
  Develop rules for double checking information - including anything you enter into the database. With the RTC project we began by double checking the accuracy of any sale in which the assessed market value was either
  10
  times greater or
  10
  times smaller than the sale price. That simple rule led us to uncover the suspect bookkeeping procedures, which led us to check the validity of all data pertaining to bundled sales.
 </p>
 <h4 id="bookmark3">
  Further education can be your friend
 </h4>
 <p>
  Many editors will recognize the need for some form of technical training, but unfortunately most will interpret that to be an exercise in producing greater hardware and software proficiency. If your ultimate goal is to produce journalism that extends beyond the generation of ranked lists, you should seriously consider enrolling in a basic statistics course. If nothing else, it is important to know the difference between a mean, a median, and a mode and proper times to use each such measurement of the “average.” If data-based journalism is to take the next leap forward, it will be on the backs of reporters and editors who understand numbers.
 </p>
 <p>
  <strong>
   36
  </strong>
 </p>
</body>
